{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Mathematics, Algorithms and Modeling\n",
    "\n",
    "# AI Powered Recipe Recommendation System \n",
    "\n",
    "### Team : Group 3\n",
    "| Student No  | First Name                  | Last Name     |\n",
    "|-------------|-----------------------------|---------------|\n",
    "| 9041129     | Nidhi                       | Ahir          |\n",
    "| 9016986     | Keerthi                     | Gonuguntla    |\n",
    "| 9027375     | Khushbu                     | Lad           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "In the next phase of recepe recommandation system, Data related to user feedback and rating are taken into consideration with the existing recepe data with a view to identify insights about user's preferences and engagement with the recepe. This will help to identify corelation between recepe characteristics and user preferences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectangular dataset : Raw_interaction.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representing the new data set in classes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawRecipe:\n",
    "    def __init__(self):\n",
    "        self.file_path = './Dataset/RAW_recipes.csv'\n",
    "        self.data = None\n",
    "    \n",
    "    # Loads the data from a CSV file.\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_csv(self.file_path)\n",
    "        print(f\"---> STEP 1 : Loads the data from a CSV file. \\r\\n\")\n",
    "        print(f\"RAW_recipes.csv : Data loaded successfully.\")\n",
    "        print(f\"Total Records : {self.data.shape[0]} \\r\\n\")\n",
    "        return self.data\n",
    "    \n",
    "class RecepeInteraction:\n",
    "    def __init__(self):\n",
    "        self.file_path = './Dataset/RAW_interactions.csv'\n",
    "        self.data = None\n",
    "    \n",
    "    # Loads the data from a CSV file.\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_csv(self.file_path)\n",
    "        print(f\"---> STEP 1 : Loads the data from a CSV file. \\r\\n\")\n",
    "        print(f\"RAW_interactions.csv : Data loaded successfully.\")\n",
    "        print(f\"Total Records : {self.data.shape[0]} \\r\\n\")\n",
    "        return self.data\n",
    "    \n",
    "    def view_sample_data(self):\n",
    "        self.data.head(5)\n",
    "\n",
    "    # Data quality : Null Check\n",
    "    def check_null_values(self):\n",
    "        print(f\"---> STEP 2 : Null Check for data \\r\\n\")\n",
    "        if self.data is not None:\n",
    "            nulls = self.data.isnull().sum()\n",
    "            print(nulls)\n",
    "            return nulls\n",
    "        else:\n",
    "            print(\"Data not loaded.\")\n",
    "     # Data quality : Duplicate Check\n",
    "    def check_duplicate_values(self):\n",
    "        print(f\"\\r\\n---> STEP 3 : Duplicate data Check for recepe \\r\\n\")\n",
    "        if self.data is not None:\n",
    "            counts = self.data[\"recipe_id\"].value_counts()\n",
    "            dupl = (counts[counts>1]).reset_index()\n",
    "            dupl.columns = [\"recipe_id\", \"Count\"]\n",
    "            print(dupl)\n",
    "            return dupl\n",
    "        else:\n",
    "            print(\"Data not loaded.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create an instance of the RecepeInteraction  class and load data\n",
    "    interactionData = RecepeInteraction()\n",
    "    interactionData.load_data()\n",
    "\n",
    "    # Create an instance of the RecepeInteraction  class and load data\n",
    "    recepeData = RawRecipe()\n",
    "    recepeData.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactionData.data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset appears to contain reviews and ratings for various recipes. Here's a breakdown of each column:\n",
    "\n",
    "**user_id:** Unique identifier for the user who provided the rating/review.\n",
    "\n",
    "**recipe_id:** Unique identifier for the recipe being rated/reviewed.\n",
    "\n",
    "**date:** Date when the rating and review were provided.\n",
    "\n",
    "**rating:** Numerical rating (on a scale of 0 to 5) given to the recipe.\n",
    "\n",
    "**review:** User's textual review providing additional feedback or modifications to the recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for missing values\n",
    "interactionData.check_null_values()\n",
    "\n",
    "# Check duplicate values\n",
    "interactionData.check_duplicate_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data using common field recepe Id\n",
    "merged_data = pd.merge(recepeData.data, interactionData.data, left_on='id', right_on='recipe_id')\n",
    "print(\"Data Merged Successfully\")\n",
    "merged_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothisis : Recipe rating is propotional to preperation time i.e more the preperation time , more ratings recepe get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQ Normal Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "minutes = recepeData.data[\"minutes\"] \n",
    "rating= interactionData.data[\"rating\"]\n",
    "\n",
    "def create_qq_plot(ax,data, title, line_color, line_width, point_color, point_size):\n",
    "    (osm, osr), (slope, intercept, r) = zscore.probplot(data, dist=\"norm\")\n",
    "    \n",
    "    # Plot the data points\n",
    "    ax.scatter(osm, osr, color=point_color, s=point_size, label='Data Points')\n",
    "    \n",
    "    # Plot the fit line\n",
    "    ax.plot(osm, slope * osm + intercept, color=line_color, lw=line_width, label='Fit Line')\n",
    "    \n",
    "    # Set title and labels for the subplot (ax)\n",
    "    ax.set_title(f'QQ Plot of {title}')\n",
    "    ax.set_xlabel('Theoretical Quantiles ' + title)\n",
    "    ax.set_ylabel('Sample Quantiles ' + title)\n",
    "    \n",
    "    # Add legend and grid\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # 1 row, 2 columns\n",
    "\n",
    "# QQ Plot for minutes\n",
    "create_qq_plot(axes[0],minutes,'minutes',\"blue\",1,\"skyblue\",3)\n",
    "\n",
    "# QQ Plot for Rating\n",
    "create_qq_plot(axes[1],rating, 'Rating',\"red\",1,\"yellow\",5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = merged_data['minutes'].corr(merged_data['rating'])\n",
    "\n",
    "print(f'Correlation coefficient between Variable1 and Variable2: {correlation}')\n",
    "\n",
    "# Optionally, visualize the relationship using a scatter plot\n",
    "# sns.scatterplot(x='id', y='rating', data=merged_data)\n",
    "# plt.title('Scatter Plot of id vs rating')\n",
    "# plt.xlabel('id')\n",
    "# plt.ylabel('ratings')\n",
    "# plt.show()\n",
    "\n",
    "df = merged_data.apply(pd.to_numeric, errors = 'coerce')\n",
    "corr_mat = df.corr()\n",
    "print(corr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zscore_steps = zscore.zscore(merged_data['rating'])\n",
    "print(f\"Z-Score for the steps\\n \",Zscore_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mean = merged_data['rating'].mean()\n",
    "sample_std = merged_data['rating'].std()\n",
    "n = len(merged_data['rating'])\n",
    "\n",
    "# Specify the population mean\n",
    "population_mean = 0 \n",
    "\n",
    "# Calculate the t-score\n",
    "t_score = (sample_mean - population_mean) / (sample_std / np.sqrt(n))\n",
    "\n",
    "print(f\"T-score for {'rating'}: {t_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk Normality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for the Shapiro-Wilk test\n",
    "numeric_columns = merged_data.select_dtypes(include='number').columns\n",
    "\n",
    "# Apply the Shapiro-Wilk test for normality on each numeric column\n",
    "shapiro_results = {}\n",
    "\n",
    "for column in numeric_columns:\n",
    "    statistic, p_value = stats.shapiro(merged_data['rating'].dropna())  # Use dropna() to remove NaN values\n",
    "    shapiro_results['rating'] = p_value\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "shapiro_results_df = pd.DataFrame(list(shapiro_results.items()), columns=['rating', 'p_value'])\n",
    "\n",
    "# Display the Shapiro-Wilk test results\n",
    "print(shapiro_results_df)\n",
    "\n",
    "numeric_columns = merged_data.select_dtypes(include='number').columns\n",
    "\n",
    "# Apply the Shapiro-Wilk test for normality on each numeric column\n",
    "shapiro_results = {}\n",
    "\n",
    "for column in numeric_columns:\n",
    "    statistic, p_value = stats.shapiro(merged_data['minutes'].dropna())  # Use dropna() to remove NaN values\n",
    "    shapiro_results['minutes'] = p_value\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "shapiro_results_df = pd.DataFrame(list(shapiro_results.items()), columns=['minutes', 'p_value'])\n",
    "\n",
    "# Display the Shapiro-Wilk test results\n",
    "print(shapiro_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here P-Values is less than 0.05 for the both ratings and minutes so that our dta is normally distributed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-TEST :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data creation (assuming these are your dataframes)\n",
    "# raw_recipes = pd.read_csv('path_to_your_recipes.csv')\n",
    "# raw_interaction = pd.read_csv('path_to_your_interaction.csv')\n",
    "\n",
    "# For demonstration, let's create dummy data\n",
    "# Assuming these are your numerical columns of interest\n",
    "\n",
    "\n",
    "# Extract the data to be tested\n",
    "group1 = merged_data['minutes']\n",
    "group2 = merged_data['rating']\n",
    "\n",
    "# Perform the F-test\n",
    "f_statistic, p_value = stats.levene(group1, group2)  # Use Levene's test for equality of variances\n",
    "\n",
    "# Display the results\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value <= alpha:\n",
    "    print(\"Reject the null hypothesis: Variances are significantly different.\")\n",
    "else:\n",
    "    print(\"Accept the null hypothesis: Variances are not significantly different.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wilcox Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = merged_data['minutes']\n",
    "data2 = merged_data['rating']\n",
    "\n",
    "# Perform Wilcoxon Signed-Rank Test\n",
    "stat, p_value = stats.wilcoxon(data1, data2)\n",
    "\n",
    "print('Wilcoxon Signed-Rank Test Statistic:', stat)\n",
    "print('P-value:', p_value)\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the two related samples.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two related samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "Ratings are NOT propotional to Preperation time "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvPROG8431",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

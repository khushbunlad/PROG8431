{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Mathematics, Algorithms and Modeling\n",
    "\n",
    "# AI Powered Recipe Recommendation System \n",
    "\n",
    "### Team : Group 3\n",
    "| Student No  | First Name                  | Last Name     |\n",
    "|-------------|-----------------------------|---------------|\n",
    "| 9041129     | Nidhi                       | Ahir          |\n",
    "| 9016986     | Keerthi                     | Gonuguntla    |\n",
    "| 9027375     | Khushbu                     | Lad           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "This presentation is focused on the data cleaning techniques as per last class notes. It will mainly deal with data reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Programming Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ractangular Dataset : files\n",
    "1. Raw_recepes.csv\n",
    "2. Raw_interaction.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import scipy.stats as zscore\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RawRecipe : Dataset in classes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawRecipe:\n",
    "    def __init__(self):\n",
    "        self.file_path = './Dataset/RAW_recipes.csv'\n",
    "        self.data = None\n",
    "    \n",
    "    # Loads the data from a CSV file.\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_csv(self.file_path)\n",
    "        print(f\"---> STEP 1 : Loads the data from a CSV file. \\r\\n\")\n",
    "        print(f\"RAW_recipes.csv : Data loaded successfully.\")\n",
    "        print(f\"Total Records : {self.data.shape[0]} \\r\\n\")\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RAW_interactions : Dataset in classes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecepeInteraction:\n",
    "    def __init__(self):\n",
    "        self.file_path = './Dataset/RAW_interactions.csv'\n",
    "        self.data = None\n",
    "    \n",
    "    # Loads the data from a CSV file.\n",
    "    def load_data(self):\n",
    "        self.data = pd.read_csv(self.file_path)\n",
    "        print(f\"---> STEP 1 : Loads the data from a CSV file. \\r\\n\")\n",
    "        print(f\"RAW_interactions.csv : Data loaded successfully.\")\n",
    "        print(f\"Total Records : {self.data.shape[0]} \\r\\n\")\n",
    "        return self.data\n",
    "    \n",
    "    def view_sample_data(self):\n",
    "        self.data.head(5)\n",
    "\n",
    "    # Data quality : Null Check\n",
    "    def check_null_values(self):\n",
    "        print(f\"---> STEP 2 : Null Check for data \\r\\n\")\n",
    "        if self.data is not None:\n",
    "            nulls = self.data.isnull().sum()\n",
    "            print(nulls)\n",
    "            return nulls\n",
    "        else:\n",
    "            print(\"Data not loaded.\")\n",
    "     # Data quality : Duplicate Check\n",
    "    def check_duplicate_values(self):\n",
    "        print(f\"\\r\\n---> STEP 3 : Duplicate data Check for recepe \\r\\n\")\n",
    "        if self.data is not None:\n",
    "            counts = self.data[\"recipe_id\"].value_counts()\n",
    "            dupl = (counts[counts>1]).reset_index()\n",
    "            dupl.columns = [\"recipe_id\", \"Count\"]\n",
    "            print(dupl)\n",
    "            return dupl\n",
    "        else:\n",
    "            print(\"Data not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main function : Initialise class objects & load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> STEP 1 : Loads the data from a CSV file. \n",
      "\n",
      "RAW_interactions.csv : Data loaded successfully.\n",
      "Total Records : 1132367 \n",
      "\n",
      "---> STEP 1 : Loads the data from a CSV file. \n",
      "\n",
      "RAW_recipes.csv : Data loaded successfully.\n",
      "Total Records : 231637 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create an instance of the RecepeInteraction  class and load data\n",
    "    interactionData = RecepeInteraction()\n",
    "    interactionData.load_data()\n",
    "\n",
    "    # Create an instance of the RecepeInteraction  class and load data\n",
    "    recepeData = RawRecipe()\n",
    "    recepeData.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataset based on recepe Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Merged Successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>minutes</th>\n",
       "      <th>contributor_id</th>\n",
       "      <th>submitted</th>\n",
       "      <th>tags</th>\n",
       "      <th>nutrition</th>\n",
       "      <th>n_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>n_ingredients</th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>137739</td>\n",
       "      <td>55</td>\n",
       "      <td>47892</td>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>['make a choice and proceed with recipe', 'dep...</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n",
       "      <td>7</td>\n",
       "      <td>4470</td>\n",
       "      <td>137739</td>\n",
       "      <td>2006-02-18</td>\n",
       "      <td>5</td>\n",
       "      <td>I used an acorn squash and recipe#137681 Swee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arriba   baked winter squash mexican style</td>\n",
       "      <td>137739</td>\n",
       "      <td>55</td>\n",
       "      <td>47892</td>\n",
       "      <td>2005-09-16</td>\n",
       "      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n",
       "      <td>[51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]</td>\n",
       "      <td>11</td>\n",
       "      <td>['make a choice and proceed with recipe', 'dep...</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n",
       "      <td>7</td>\n",
       "      <td>593927</td>\n",
       "      <td>137739</td>\n",
       "      <td>2010-08-21</td>\n",
       "      <td>5</td>\n",
       "      <td>This was a nice change. I used butternut squas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name      id  minutes  \\\n",
       "0  arriba   baked winter squash mexican style  137739       55   \n",
       "1  arriba   baked winter squash mexican style  137739       55   \n",
       "\n",
       "   contributor_id   submitted  \\\n",
       "0           47892  2005-09-16   \n",
       "1           47892  2005-09-16   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "1  ['60-minutes-or-less', 'time-to-make', 'course...   \n",
       "\n",
       "                               nutrition  n_steps  \\\n",
       "0  [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]       11   \n",
       "1  [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0]       11   \n",
       "\n",
       "                                               steps  \\\n",
       "0  ['make a choice and proceed with recipe', 'dep...   \n",
       "1  ['make a choice and proceed with recipe', 'dep...   \n",
       "\n",
       "                                         description  \\\n",
       "0  autumn is my favorite time of year to cook! th...   \n",
       "1  autumn is my favorite time of year to cook! th...   \n",
       "\n",
       "                                         ingredients  n_ingredients  user_id  \\\n",
       "0  ['winter squash', 'mexican seasoning', 'mixed ...              7     4470   \n",
       "1  ['winter squash', 'mexican seasoning', 'mixed ...              7   593927   \n",
       "\n",
       "   recipe_id        date  rating  \\\n",
       "0     137739  2006-02-18       5   \n",
       "1     137739  2010-08-21       5   \n",
       "\n",
       "                                              review  \n",
       "0   I used an acorn squash and recipe#137681 Swee...  \n",
       "1  This was a nice change. I used butternut squas...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge data using common field recepe Id\n",
    "merged_data = pd.merge(recepeData.data, interactionData.data, left_on='id', right_on='recipe_id')\n",
    "print(\"Data Merged Successfully\")\n",
    "merged_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing value Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Ratio</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name</td>\n",
       "      <td>0.0000008831</td>\n",
       "      <td>0.0000883106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minutes</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>contributor_id</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>submitted</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tags</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nutrition</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n_steps</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>steps</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>description</td>\n",
       "      <td>0.0207618202</td>\n",
       "      <td>2.0761820152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ingredients</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>n_ingredients</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>user_id</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>recipe_id</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>date</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rating</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review</td>\n",
       "      <td>0.0001492449</td>\n",
       "      <td>0.0149244900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Column Name        Ratio   Percentage\n",
       "0             name 0.0000008831 0.0000883106\n",
       "1               id 0.0000000000 0.0000000000\n",
       "2          minutes 0.0000000000 0.0000000000\n",
       "3   contributor_id 0.0000000000 0.0000000000\n",
       "4        submitted 0.0000000000 0.0000000000\n",
       "5             tags 0.0000000000 0.0000000000\n",
       "6        nutrition 0.0000000000 0.0000000000\n",
       "7          n_steps 0.0000000000 0.0000000000\n",
       "8            steps 0.0000000000 0.0000000000\n",
       "9      description 0.0207618202 2.0761820152\n",
       "10     ingredients 0.0000000000 0.0000000000\n",
       "11   n_ingredients 0.0000000000 0.0000000000\n",
       "12         user_id 0.0000000000 0.0000000000\n",
       "13       recipe_id 0.0000000000 0.0000000000\n",
       "14            date 0.0000000000 0.0000000000\n",
       "15          rating 0.0000000000 0.0000000000\n",
       "16          review 0.0001492449 0.0149244900"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_value_ratio = (merged_data.isnull().sum() / len(merged_data))\n",
    "missing_value_percentage = missing_value_ratio * 100\n",
    "pd.set_option('display.float_format', '{:.10f}'.format)\n",
    "\n",
    "missing_values_table = pd.DataFrame({\n",
    "    'Column Name': missing_value_ratio.index,\n",
    "    'Ratio': missing_value_ratio.values,\n",
    "    'Percentage': missing_value_percentage.values\n",
    "})\n",
    "missing_values_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data suggests that there are only null values in 3 columns name, description and review. Here, description and review is not a mandatory or important feature in the dataset. The \"name\" column must have value. We can drop records with null names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.dropna(subset=['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Variance Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                      17003803127.6721191406\n",
      "minutes              77378371288826.5937500000\n",
      "contributor_id     4589619737965439.0000000000\n",
      "n_steps                          33.8687988906\n",
      "n_ingredients                    13.6154322700\n",
      "user_id          251429104826142400.0000000000\n",
      "recipe_id               17003803127.6721191406\n",
      "rating                            1.5995814482\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "numerical_features = merged_data.select_dtypes(include=['number'])\n",
    "variance = numerical_features.var()\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reviewing variance of all numerical features, features with variance > 100 are columns including ID's. \n",
    "2. number of steps and ingredients has considerable lower variance, however those columns contains values which can not be ignored to perform ML tasks.\n",
    "3. Rating is having lowest variance among all numerical features. However, Rating is integral part of data as it has categorical data between 1 to 5 numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Correlation Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id      minutes  contributor_id      n_steps  n_ingredients  \\\n",
      "id             NaN 0.0031643677    0.1029516836 0.0567002909   0.0181197736   \n",
      "minutes        NaN          NaN    0.0001325212 0.0004375931   0.0010589317   \n",
      "contributor_id NaN          NaN             NaN 0.0277206877   0.0053791485   \n",
      "n_steps        NaN          NaN             NaN          NaN   0.3802954944   \n",
      "n_ingredients  NaN          NaN             NaN          NaN            NaN   \n",
      "user_id        NaN          NaN             NaN          NaN            NaN   \n",
      "recipe_id      NaN          NaN             NaN          NaN            NaN   \n",
      "rating         NaN          NaN             NaN          NaN            NaN   \n",
      "\n",
      "                    user_id    recipe_id       rating  \n",
      "id             0.1000590419 1.0000000000 0.0135653999  \n",
      "minutes        0.0005947115 0.0031643677 0.0010534285  \n",
      "contributor_id 0.1027458073 0.1029516836 0.0122144900  \n",
      "n_steps        0.0516858362 0.0567002909 0.0211706262  \n",
      "n_ingredients  0.0081342176 0.0181197736 0.0035286409  \n",
      "user_id                 NaN 0.1000590419 0.1961822884  \n",
      "recipe_id               NaN          NaN 0.0135653999  \n",
      "rating                  NaN          NaN          NaN  \n",
      "Features to remove due to high correlation: ['recipe_id']\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = numerical_features.corr().abs()\n",
    "upper_triangle = correlation_matrix.where(\n",
    "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "print(upper_triangle)\n",
    "\n",
    "threshold = 0.9\n",
    "highly_correlated = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "\n",
    "print(\"Features to remove due to high correlation:\", highly_correlated)\n",
    "\n",
    "df_filtered = merged_data.drop(columns=highly_correlated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.25726245 0.1719967 ]\n",
      "Principal Components:\n",
      "                   PC1           PC2\n",
      "0       -0.3247712722 -0.2419663842\n",
      "1       -0.3245840519 -0.2418344166\n",
      "2       -0.3247160209 -0.2419274387\n",
      "3       -1.2904069975 -0.1049440968\n",
      "4       -1.4926938349 -0.4891884359\n",
      "...               ...           ...\n",
      "1132361  1.4020954602 -0.8122966214\n",
      "1132362  1.5375237071  0.1228669409\n",
      "1132363  1.4096947961 -0.9129287057\n",
      "1132364  1.2480004672 -1.2202289191\n",
      "1132365  2.0226483570 -0.4325397473\n",
      "\n",
      "[1132366 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extracting only numeric columns for PCA\n",
    "numeric_data = merged_data.select_dtypes(include=[float, int])\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(numeric_data)\n",
    "\n",
    "# Applying PCA, choosing 2 components as an example\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Creating a DataFrame with the principal components\n",
    "pca_data = pd.DataFrame(principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Displaying the explained variance and the transformed data\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Principal Components:\\n\", pca_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances:\n",
      " user_id          0.3792770853\n",
      "contributor_id   0.1524160286\n",
      "id               0.1005026234\n",
      "recipe_id        0.1004300101\n",
      "minutes          0.1004252063\n",
      "n_steps          0.0899135655\n",
      "n_ingredients    0.0770354808\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def random_forest_feature_importance(data, target_column='rating'):\n",
    "    \"\"\"\n",
    "    Use Random Forest to evaluate feature importance.\n",
    "    \"\"\"\n",
    "    # Selecting the features and target variable\n",
    "    X = data.select_dtypes(include=['number']).drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Instantiate the RandomForestRegressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "    feature_importances_sorted = feature_importances.sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Feature importances:\\n\", feature_importances_sorted)\n",
    "    return feature_importances_sorted\n",
    "\n",
    "# Apply Random Forest Feature Importance to the cleaned merged data\n",
    "feature_importances = random_forest_feature_importance(merged_data, target_column='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward/Forward Feature Elimination/Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected after Backward Elimination: Index(['id', 'minutes', 'contributor_id', 'n_steps', 'n_ingredients',\n",
      "       'user_id', 'recipe_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def backward_elimination(data, target_column='rating', threshold=0.05):\n",
    "   \n",
    "    # Define the target and features\n",
    "    X = data.select_dtypes(include=['number']).drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Instantiate the RandomForestRegressor\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "    \n",
    "    # Backward Elimination: Remove features with low importance\n",
    "    while feature_importances.max() < threshold:\n",
    "        # Remove the least important feature\n",
    "        least_important_feature = feature_importances.idxmin()\n",
    "        X_train = X_train.drop(columns=[least_important_feature])\n",
    "        X_test = X_test.drop(columns=[least_important_feature])\n",
    "        \n",
    "        # Re-train the model\n",
    "        rf.fit(X_train, y_train)\n",
    "        feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    \n",
    "    print(f\"Features selected after Backward Elimination: {X_train.columns}\")\n",
    "    return X_train, X_test\n",
    "\n",
    "# Apply Backward Elimination\n",
    "X_train_selected, X_test_selected = backward_elimination(merged_data, target_column='rating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['minutes']\n",
      "Selected features: ['minutes', 'n_ingredients']\n",
      "Selected features: ['minutes', 'n_ingredients', 'n_steps']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m selected_features\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Apply Forward Selection\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m selected_features_forward \u001b[38;5;241m=\u001b[39m \u001b[43mforward_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 26\u001b[0m, in \u001b[0;36mforward_selection\u001b[1;34m(data, target_column, threshold)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m remaining_features:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Train a RandomForest model using current features\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Evaluate model performance (mean squared error)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test[selected_features \u001b[38;5;241m+\u001b[39m [feature]])\n",
      "File \u001b[1;32md:\\Conestoga\\PROG8431-DAM\\PROG8431\\venvPROG8431\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Conestoga\\PROG8431-DAM\\PROG8431\\venvPROG8431\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32md:\\Conestoga\\PROG8431-DAM\\PROG8431\\venvPROG8431\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Conestoga\\PROG8431-DAM\\PROG8431\\venvPROG8431\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32md:\\Conestoga\\PROG8431-DAM\\PROG8431\\venvPROG8431\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32md:\\Conestoga\\PROG8431-DAM\\PROG8431\\venvPROG8431\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Conestoga\\PROG8431-DAM\\PROG8431\\venvPROG8431\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32md:\\Conestoga\\PROG8431-DAM\\PROG8431\\venvPROG8431\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def forward_selection(data, target_column='rating', threshold=0.05):\n",
    "    \"\"\"\n",
    "    Perform Forward Selection based on model performance.\n",
    "    Adds features one by one, selecting the feature that improves the model most.\n",
    "    \"\"\"\n",
    "    # Define the target and features\n",
    "    X = data.select_dtypes(include=['number']).drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    selected_features = []\n",
    "    remaining_features = X_train.columns.tolist()\n",
    "    \n",
    "    while len(remaining_features) > 0:\n",
    "        feature_performance = {}\n",
    "        \n",
    "        # Evaluate each feature\n",
    "        for feature in remaining_features:\n",
    "            # Train a RandomForest model using current features\n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train[selected_features + [feature]], y_train)\n",
    "            \n",
    "            # Evaluate model performance (mean squared error)\n",
    "            y_pred = model.predict(X_test[selected_features + [feature]])\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            feature_performance[feature] = mse\n",
    "        \n",
    "        # Select the feature with the best (lowest) MSE\n",
    "        best_feature = min(feature_performance, key=feature_performance.get)\n",
    "        \n",
    "        # Add the best feature to the selected features list\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "        \n",
    "        print(f\"Selected features: {selected_features}\")\n",
    "    \n",
    "    print(f\"Final selected features after Forward Selection: {selected_features}\")\n",
    "    return selected_features\n",
    "\n",
    "# Apply Forward Selection\n",
    "selected_features_forward = forward_selection(merged_data, target_column='rating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After feature selection, you can train a model using the selected features:\n",
    "selected_features = selected_features_forward  # Or selected_features_from_backward_elimination\n",
    "\n",
    "# Using selected features to train the final model\n",
    "X_train_final = X_train[selected_features]\n",
    "X_test_final = X_test[selected_features]\n",
    "\n",
    "# Instantiate and train RandomForestRegressor\n",
    "final_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = final_model.predict(X_test_final)\n",
    "final_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Final Model Mean Squared Error: {final_mse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvPROG8431",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
